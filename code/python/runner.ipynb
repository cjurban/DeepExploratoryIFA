{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Jupyter Notebook for *A Deep Learning Algorithm for High-Dimensional Exploratory Item Factor Analysis*\n",
    "\n",
    "### Journal: Psychometrika\n",
    "### Authors: Christopher J. Urban and Daniel J. Bauer\n",
    "### Affil.: L. L. Thurstone Psychometric Laboratory in the Dept. of Psychology and Neuroscience, UNC-Chapel Hill\n",
    "### E-mail: cjurban@live.unc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for conducting amortized importance-weighted variational inference for exploratory IFA.\n",
    "\n",
    "First, import necessary packages, then runtime parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import sys\n",
    "import timeit\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from pylab import *\n",
    "import subprocess\n",
    "\n",
    "from utils import *\n",
    "from helper_layers import *\n",
    "from base_class import *\n",
    "from mirt_vae import *\n",
    "from read_data import *\n",
    "from cross_validation import *\n",
    "from simulations import *\n",
    "\n",
    "# Re-import some packages to reload functions without needing to restart the kernel.\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules[\"utils\"])\n",
    "importlib.reload(sys.modules[\"helper_layers\"])\n",
    "importlib.reload(sys.modules[\"base_class\"])\n",
    "importlib.reload(sys.modules[\"mirt_vae\"])\n",
    "importlib.reload(sys.modules[\"read_data\"])\n",
    "importlib.reload(sys.modules[\"cross_validation\"])\n",
    "importlib.reload(sys.modules[\"simulations\"])\n",
    "\n",
    "from utils import *\n",
    "from helper_layers import *\n",
    "from base_class import *\n",
    "from mirt_vae import *\n",
    "from read_data import *\n",
    "from cross_validation import *\n",
    "from simulations import *\n",
    "\n",
    "# Suppress scientific notation.\n",
    "np.set_printoptions(suppress = True)\n",
    "\n",
    "# Print full arrays.\n",
    "np.set_printoptions(threshold = sys.maxsize)\n",
    "\n",
    "# If CUDA is available, use it.\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "kwargs = {\"num_workers\" : 1, \"pin_memory\" : True} if cuda else {}\n",
    "\n",
    "# Set log intervals.\n",
    "ipip_log_interval = 200\n",
    "sim_log_interval = 75\n",
    "\n",
    "base_path = \"\" # Base path goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPIP-FFM Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make IPIP-FFM data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipip_filename = base_path + \"data/ipip_ffm/ipip_ffm_recoded.csv\"\n",
    "\n",
    "# Full data set.\n",
    "ipip_loader = torch.utils.data.DataLoader(\n",
    "    ipip_ffm_dataset(csv_file = ipip_filename,\n",
    "                     which_split = \"full\",\n",
    "                     transform = to_tensor()),\n",
    "    batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "# Test data set.\n",
    "ipip_test_loader = torch.utils.data.DataLoader(\n",
    "    ipip_ffm_dataset(csv_file = ipip_filename,\n",
    "                     which_split = \"test-only\",\n",
    "                     transform = to_tensor()),\n",
    "    batch_size = 32, shuffle = True, **kwargs)\n",
    "\n",
    "\n",
    "# Train data set.\n",
    "ipip_train_loader = torch.utils.data.DataLoader(\n",
    "    ipip_ffm_dataset(csv_file = ipip_filename,\n",
    "                     which_split = \"train-only\",\n",
    "                     transform = to_tensor()),\n",
    "    batch_size = 32, shuffle = True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model with many random starts and save fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory to save results.\n",
    "res_path = base_path + \"results/ipip_ffm/random_starts/\"\n",
    "Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "for rep in range(100):\n",
    "    print(\"Starting replication\", rep)\n",
    "    \n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(rep)\n",
    "    np.random.seed(rep)\n",
    "\n",
    "    # Initialize model.\n",
    "    start = timeit.default_timer()\n",
    "    ipip_vae = MIRTVAEClass(input_dim = 250,\n",
    "                            inference_model_dims = [130],\n",
    "                            latent_dim = 5,\n",
    "                            n_cats = [5]*50,\n",
    "                            learning_rate = 5e-3,\n",
    "                            device = device,\n",
    "                            log_interval = ipip_log_interval,\n",
    "                            steps_anneal = 1000)\n",
    "\n",
    "    # Fit model.\n",
    "    ipip_vae.run_training(ipip_loader, ipip_test_loader, iw_samples = 5)\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    # Save run time.\n",
    "    Path(res_path + \"run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"/run_times/run_time_\" + str(rep) + \".txt\",\n",
    "               np.array([stop - start]), \n",
    "               delimiter = \",\",\n",
    "               fmt = \"%f\")\n",
    "\n",
    "    # Save model loadings.\n",
    "    Path(res_path + \"loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"loadings/loadings_\" + str(rep) + \".txt\",\n",
    "               ipip_vae.model.loadings.weight.data.numpy(),\n",
    "               fmt='%f')\n",
    "\n",
    "    # Save model intercepts.\n",
    "    Path(res_path + \"intercepts/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"intercepts/intercepts_\" + str(rep) + \".txt\",\n",
    "               ipip_vae.model.intercepts.bias.data.numpy(),\n",
    "               fmt = \"%f\")\n",
    "    \n",
    "    # Save model log-likelihood.\n",
    "    ll = -ipip_vae.bic(ipip_test_loader,\n",
    "                       iw_samples = 5000)[1]\n",
    "    Path(res_path + \"lls/\").mkdir(parents = True, exist_ok = True)\n",
    "    np.savetxt(res_path + \"lls/ll_\" + str(rep) + \".txt\",\n",
    "               np.array([ll]), \n",
    "               delimiter = \",\",\n",
    "               fmt = \"%f\")\n",
    "    \n",
    "    print(\"Stored results for replication\", rep)\n",
    "\n",
    "# Rotate factor loadings.\n",
    "subprocess.call(base_path + \"code/r/rotate_ipip_ffm.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predicted approximate log-likelihood for different latent dimensions P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_ls = []\n",
    "latent_dims = np.arange(1, 11).tolist()\n",
    "for latent_dim in latent_dims:\n",
    "    print(\"Starting fitting for P =\", latent_dim)\n",
    "    \n",
    "    # Set random seeds.\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # Initialize model.\n",
    "    start = timeit.default_timer()\n",
    "    ipip_vae = MIRTVAEClass(input_dim = 250,\n",
    "                            inference_model_dims = [int((250 + 2 * latent_dim) / 2)],\n",
    "                            latent_dim = latent_dim,\n",
    "                            n_cats = [5]*50,\n",
    "                            learning_rate = 5e-3,\n",
    "                            device = device,\n",
    "                            log_interval = ipip_log_interval,\n",
    "                            steps_anneal = 1000)\n",
    "\n",
    "    # Fit model.\n",
    "    ipip_vae.run_training(ipip_train_loader, ipip_test_loader, iw_samples = 5)\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    print(\"Model fitted, run time =\", stop - start, \"seconds\")\n",
    "    \n",
    "    # Save model log-likelihood.\n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    start = timeit.default_timer()\n",
    "    ll_ls.append(-ipip_vae.bic(ipip_test_loader, iw_samples = 5000)[1])\n",
    "    stop = timeit.default_timer()\n",
    "    \n",
    "    print(\"LL stored, computed in\", stop - start, \"seconds\")\n",
    "    \n",
    "for idx, dim in enumerate(latent_dims):\n",
    "    print(\"Latent dimension =\", dim, \"LL =\", ll_ls[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(constrained_layout = True)\n",
    "fig.set_size_inches(5, 5, forward = True)\n",
    "fig.set_size_inches(5, 5, forward = True)\n",
    "ax.axvline(x = 5, color = \"gray\", linestyle = \"--\")\n",
    "ax.plot(np.arange(1, 11).tolist(), [ll for ll in ll_ls], \"k-o\")\n",
    "ax.set_xticks(np.arange(1, 11).tolist())\n",
    "ax.set_ylabel(\"Predicted Approximate Negative Log-Likelihood\")\n",
    "ax.set_xlabel(\"Number of Factors\")\n",
    "fig.suptitle(\"Approximate Log-Likelihood Plot for IPIP-FFM Data\")\n",
    "fig.show()\n",
    "\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/scree_plot_ipip_ffm.pdf\")\n",
    "pdf.savefig(fig, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute mean RMSEs for fitted values across random starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in log-likelihood filenames.\n",
    "res_path = base_path + \"results/ipip_ffm/random_starts/\"\n",
    "ll_filenames = os.listdir(res_path + \"lls/\")\n",
    "\n",
    "# Read in fitted values.\n",
    "lls        = [np.loadtxt(res_path + \"lls/ll_\" + str(num) + \".txt\", dtype = float).item() for\n",
    "              num in range(len(ll_filenames))]\n",
    "idxs       = [num for num in range(len(lls)) if not np.isnan(lls[num])]\n",
    "loadings   = [np.loadtxt(res_path + \"rotated_loadings/rotated_loadings_\" + str(num) + \".txt\", dtype = float) for\n",
    "              num in range(len(ll_filenames)) if not np.isnan(lls[num])]\n",
    "intercepts = [np.loadtxt(res_path + \"intercepts/intercepts_\" + str(num) + \".txt\", dtype = float) for\n",
    "              num in range(len(ll_filenames)) if not np.isnan(lls[num])]\n",
    "covs       = [np.loadtxt(res_path + \"covs/cov_\" + str(num) + \".txt\", dtype = float) for\n",
    "              num in range(len(ll_filenames)) if not np.isnan(lls[num])]\n",
    "run_times  = [np.loadtxt(res_path + \"run_times/\" + \"run_time_\" + str(num) + \".txt\",\n",
    "                         dtype = float, skiprows = 1).sum() for\n",
    "              num in range(len(ll_filenames)) if not np.isnan(lls[num])]\n",
    "\n",
    "# Obtain reference values.\n",
    "best_idx = lls.index(min(lls))\n",
    "ref_ldgs, ref_ints, ref_cov = loadings.pop(best_idx), intercepts.pop(best_idx), covs.pop(best_idx)\n",
    "best_idx = idxs.pop(best_idx)\n",
    "\n",
    "# Find first 100 converged solutions.\n",
    "n_keeps    = 100\n",
    "keeps      = [all([c > 0.98 for c in permuted_congruence(ref_ldgs, ldgs, mean = False)]) for ldgs in loadings]\n",
    "loadings   = [ldgs for ldgs, keep in zip(loadings, keeps) if keep][0:n_keeps]\n",
    "intercepts = [ints for ints, keep in zip(intercepts, keeps) if keep][0:n_keeps]\n",
    "covs       = [cov for cov, keep in zip(covs, keeps) if keep][0:n_keeps]\n",
    "run_times  = [run_time for run_time, keep in zip(run_times, keeps) if keep][0:n_keeps]\n",
    "\n",
    "# Calculate loadings RMSEs.\n",
    "loadings_biases  = [permuted_biases(ref_ldgs, ldgs) for ldgs in loadings]\n",
    "loadings_rmses   = np.sqrt(reduce(np.add, [bias**2 for bias in loadings_biases]) / len(loadings_biases))\n",
    "\n",
    "# Calculate intercepts RMSEs.\n",
    "n_cats = [5]*50\n",
    "intercepts_biases = [normalize_ints(ref_ints, ref_ldgs, n_cats) - normalize_ints(ints, ldgs, n_cats) for\n",
    "                     ints, ldgs in zip(intercepts, loadings)]\n",
    "intercepts_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in intercepts_biases]) / len(intercepts_biases))\n",
    "\n",
    "# Calculate covariance RMSEs.\n",
    "covariance_biases = [cov_biases(ref_cov, perm_cov, ref_ldgs, perm_ldgs) for\n",
    "                     perm_cov, perm_ldgs in zip(covs, loadings)]\n",
    "covariance_rmses  = np.sqrt(reduce(np.add, [bias**2 for bias in covariance_biases]) / len(covariance_biases))\n",
    "\n",
    "print(\"Mean Loadings RMSE: {:.3f} ({:.3f})\".format(np.mean(loadings_rmses),\n",
    "                                                   np.std(loadings_rmses)))\n",
    "print(\"Mean Intercepts RMSE: {:.3f} ({:.3f})\".format(np.mean(intercepts_rmses),\n",
    "                                                     np.std(intercepts_rmses)))\n",
    "print(\"Mean Covariance Matrix RMSE: {:.3f} ({:.3f})\".format(np.mean(covariance_rmses),\n",
    "                                                            np.std(covariance_rmses)))\n",
    "print(\"Mean Run Time: {:.2f} ({:.2f})\".format(np.mean(run_times),\n",
    "                                              np.std(run_times)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving Model Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save loadings estimates in normal metric and factor correlation matrix for paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path   = base_path + \"results/ipip_ffm/random_starts/\"\n",
    "loadings   = np.loadtxt(res_path + \"rotated_loadings/rotated_loadings_\" + str(best_idx) + \".txt\",\n",
    "                        dtype = float)\n",
    "intercepts = np.loadtxt(res_path + \"intercepts/intercepts_\" + str(best_idx) + \".txt\",\n",
    "                        dtype = float)\n",
    "cov        = np.loadtxt(res_path + \"covs/cov_\" + str(best_idx) + \".txt\",\n",
    "                        dtype = float)\n",
    "\n",
    "# Save loadings.\n",
    "np.savetxt(base_path + \"results/ipip_ffm/loadings_for_paper.txt\",\n",
    "           np.around(normalize_loadings(loadings), decimals = 2), \n",
    "           delimiter = \" & \",\n",
    "           fmt = \"%1.2f\")\n",
    "\n",
    "# Save loadings heatmap.\n",
    "loadings = np.loadtxt(base_path + \"results/ipip_ffm/loadings_for_paper.txt\",\n",
    "                      delimiter = \"&\")[:, [0, 3, 2, 4, 1]]\n",
    "c = pcolor(invert_factors(loadings))\n",
    "set_cmap(\"gray_r\")\n",
    "colorbar() \n",
    "c = pcolor(invert_factors(loadings), edgecolors = \"w\", linewidths = 1, vmin = 0) \n",
    "xlabel(\"Factor\")\n",
    "ylabel(\"Item\")\n",
    "xticks(np.arange(loadings.shape[1]) + 0.5,\n",
    "       [\"EXT\", \"EST\", \"AGR\", \"CON\", \"OPN\"])\n",
    "yticks(np.array([10, 20, 30, 40, 50]) - 0.5, [\"10\", \"20\", \"30\", \"40\", \"50\"])\n",
    "plt.gca().invert_xaxis()\n",
    "suptitle(\"IPIP-FFM Factor Loadings\", y = 0.93)\n",
    "savefig(base_path + \"figures/loadings_heatmap.pdf\")\n",
    "\n",
    "# Save correlation matrix.    \n",
    "np.savetxt(base_path + \"results/ipip_ffm/cors_for_paper.txt\",\n",
    "           np.around(cov, decimals = 2),\n",
    "           delimiter = \" & \",\n",
    "           fmt = \"%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save loadings matrix, intercept vectors, factor mean vector, and factor covariance matrix for simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a sparse loadings matrix and save it.\n",
    "ldgs = np.around(ref_ldgs, decimals = 1)\n",
    "nonzero_row_idxs = [[] for _ in range(ldgs.shape[1])] \n",
    "factor_idx = 0 \n",
    "for row in range(0, ldgs.shape[0]): \n",
    "    nonzero_row_idxs[factor_idx].append(np.where(abs(ldgs[row, :]) == max(abs(ldgs[row, :])))[0].item()) \n",
    "    factor_idx += (row % 10 == 9)\n",
    "nonzero_row_idxs = [Counter(ls).most_common()[0][0] for ls in nonzero_row_idxs]\n",
    "factor_idx = 0 \n",
    "for row in range(0, ldgs.shape[0]): \n",
    "    ldgs[row, np.delete(np.arange(ldgs.shape[1]), nonzero_row_idxs[factor_idx])] = 0\n",
    "    factor_idx += (row % 10 == 9)\n",
    "np.savetxt(base_path + \"data/simulations/loadings.txt\",\n",
    "           ldgs, \n",
    "           delimiter = \",\",\n",
    "           fmt = \"%1.1f\")\n",
    "\n",
    "# Save intercepts.\n",
    "np.savetxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "           np.around(ref_ints, decimals = 1),\n",
    "           delimiter = \",\",\n",
    "           fmt = \"%1.1f\")\n",
    "\n",
    "# Save covariance matrix.    \n",
    "np.savetxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "           np.around(ref_cov, decimals = 2),\n",
    "           delimiter = \",\",\n",
    "           fmt = \"%1.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Data Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation 1: Evaluating Consistency and Efficiency of the Variational Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit simulated data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings   = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/loadings.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_intercepts = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_cov        = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "\n",
    "# Set simulation cell parameters.\n",
    "n_obs_ls         = [500, 1000, 2000, 10000]\n",
    "iw_samples_ls    = [1, 5, 25]\n",
    "\n",
    "# Loop across simulation cells.\n",
    "for obs_idx, n_obs in enumerate(n_obs_ls):\n",
    "    for iw_idx, iw_samples in enumerate(iw_samples_ls):\n",
    "        # Print simulation cell.\n",
    "        print(\"Starting replications for simulation cell \" + str(int(iw_idx + 1)) + \", \" + str(int(obs_idx + 1)))\n",
    "\n",
    "        # Try to load replication results.\n",
    "        sim_cell = str(int(iw_idx + 1)) + \"_\" + str(int(obs_idx + 1))\n",
    "        cell_path = base_path + \"results/sim_1/cell_\" + sim_cell\n",
    "        Path(cell_path).mkdir(parents = True, exist_ok = True)\n",
    "        cell_res_path = cell_path + \"/cell_results\"\n",
    "        Path(cell_res_path).mkdir(parents = True, exist_ok = True)\n",
    "        cell_filename = cell_res_path + \"/cell_res\"\n",
    "        try:\n",
    "            cell_res = load_obj(cell_filename)\n",
    "        # If the replication results don't exist, conduct replications.\n",
    "        except FileNotFoundError:\n",
    "            cell_res = run_replications(n_reps = 100,\n",
    "                                        n_obs = n_obs,\n",
    "                                        cov = ref_cov,\n",
    "                                        loadings = ref_loadings,\n",
    "                                        intercepts = ref_intercepts,\n",
    "                                        batch_size = 32,\n",
    "                                        input_dim = 250,\n",
    "                                        inference_model_dims = [130],\n",
    "                                        latent_dim = 5,\n",
    "                                        n_cats = [5]*50,\n",
    "                                        learning_rate = 5e-3,\n",
    "                                        device = device,\n",
    "                                        log_interval = sim_log_interval,\n",
    "                                        steps_anneal = 1000,\n",
    "                                        kwargs = kwargs,\n",
    "                                        iw_samples = iw_samples)\n",
    "\n",
    "            # Save cell results.\n",
    "            save_obj(cell_res, cell_filename)\n",
    "            print(\"Saved results for simulation cell \" + str(int(iw_idx + 1)) + \", \" + str(int(obs_idx + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bias, MSE, fitting time, and scree plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings   = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/loadings.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_intercepts = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_cov        = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "\n",
    "res_path = base_path + \"results/sim_1/\"\n",
    "res_dirnames = [\"cell_1_1\", \"cell_1_2\", \"cell_1_3\", \"cell_1_4\",\n",
    "                \"cell_2_1\", \"cell_2_2\", \"cell_2_3\", \"cell_2_4\",\n",
    "                \"cell_3_1\", \"cell_3_2\", \"cell_3_3\", \"cell_3_4\"]\n",
    "\n",
    "# Extract factor loadings and run times.\n",
    "for dirname in res_dirnames:\n",
    "    cell_res = load_obj(res_path + dirname + \"/cell_results/cell_res\")\n",
    "    \n",
    "    # Save loadings.\n",
    "    Path(res_path + dirname + \"/loadings/\").mkdir(parents = True, exist_ok = True)\n",
    "    for idx, loadings in enumerate(cell_res[\"loadings\"]):\n",
    "        np.savetxt(res_path + dirname + \"/loadings/loadings_\" + str(idx) + \".txt\",\n",
    "                   loadings,\n",
    "                   delimiter = \",\")\n",
    "    \n",
    "    # Save run times.\n",
    "    Path(res_path + dirname + \"/run_times/\").mkdir(parents = True, exist_ok = True)\n",
    "    for idx, run_time in enumerate(cell_res[\"run_time\"]):\n",
    "        np.savetxt(res_path + dirname + \"/run_times/run_time_\" + str(idx) + \".txt\",\n",
    "                   np.array(run_time, ndmin = 1),\n",
    "                   delimiter = \",\", fmt = \"%f\")\n",
    "\n",
    "# Rotate factor loadings.\n",
    "subprocess.call(base_path + \"code/r/rotate_sim_1.R\")\n",
    "\n",
    "# Get rotated loadings and covariance matrices.\n",
    "cell_res_ls = []\n",
    "for dirname in res_dirnames:\n",
    "    cell_res = load_obj(res_path + dirname + \"/cell_results/cell_res\")\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + dirname + \"/rotated_loadings/rotated_loadings_\" + str(i) + \".txt\",\n",
    "                                       dtype = float) for i in range(100)]\n",
    "    cell_res[\"cov\"] = [np.loadtxt(res_path + dirname + \"/covs/cov_\" + str(i) + \".txt\",\n",
    "                                       dtype = float) for i in range(100)]\n",
    "    rot_mats = [np.loadtxt(res_path + dirname + \"/rot_mats/rot_mat_\" + str(i) + \".txt\",\n",
    "                           dtype = float) for i in range(100)]\n",
    "    cell_res[\"scores\"] = [torch.matmul(torch.from_numpy(rot_mats[i]).T.float(),\n",
    "                                       torch.from_numpy(cell_res[\"scores\"][i]).T).T.numpy() for\n",
    "                          i in range(100)]\n",
    "    cell_res[\"run_time\"] = [np.loadtxt(res_path + dirname + \"/run_times/run_time_\" + str(i) + \".txt\",\n",
    "                                       dtype = float, skiprows = 1).sum() for i in range(100)]\n",
    "    cell_res_ls.append(cell_res)\n",
    "\n",
    "bias = bias_boxplots(cell_res_ls = cell_res_ls,\n",
    "                     cov = ref_cov,\n",
    "                     loadings = ref_loadings,\n",
    "                     intercepts = ref_intercepts,\n",
    "                     n_cats = [5]*50,\n",
    "                     ldgs_lims = [-.025, .025],\n",
    "                     cov_lims = [-.04, .04],\n",
    "                     int_lims = [-0.13, 0.11],\n",
    "                     power = 1)\n",
    "mse = bias_boxplots(cell_res_ls = cell_res_ls,\n",
    "                    cov = ref_cov,\n",
    "                    loadings = ref_loadings,\n",
    "                    intercepts = ref_intercepts,\n",
    "                    n_cats = [5]*50,\n",
    "                    power = 2,\n",
    "                    ldgs_lims = [0, 0.0035],\n",
    "                    cov_lims = [0, 0.0035],\n",
    "                    int_lims = [0, 0.055])\n",
    "bias.show(); mse.show()\n",
    "\n",
    "time = time_plot(cell_res_ls = cell_res_ls,\n",
    "                 y_lims = [0, 250])\n",
    "time.show()\n",
    "\n",
    "scree = scree_plots(cell_res_ls = cell_res_ls)\n",
    "scree.show()\n",
    "\n",
    "# Save plots to a single PDF.\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/bias_plots_sim_1.pdf\")\n",
    "pdf.savefig(bias, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/mse_plots_sim_1.pdf\")\n",
    "pdf.savefig(mse, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/time_plots_sim_1.pdf\")\n",
    "pdf.savefig(time, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/scree_plots_sim_1.pdf\")\n",
    "pdf.savefig(scree, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute factor score correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cell_res_ls)):\n",
    "    print(\"Simulation Cell \", int(np.floor((i % 12) / 4) + 1), (i % 4) + 1)\n",
    "    cors = score_cors(cell_res_ls[i], ref_loadings)\n",
    "    # One can inspect these correlations more closely -- I print the mean and SD here for brevity.\n",
    "    print(\"Mean Correlation: \", np.mean([np.mean(abs(cor)) for cor in cors]))\n",
    "    print(\"St. Dev. Correlation: \", np.std([np.mean(abs(cor)) for cor in cors])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation 2: Comparing Estimator Efficiency in the Classical Asymptotic Regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save simulated data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings   = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/loadings.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_intercepts = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_cov        = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "\n",
    "factor_mul_ls = [2, 2, 2, 2]\n",
    "item_mul_ls   = [1, 1, 1, 1]\n",
    "n_obs_ls      = [1000, 2000, 5000, 10000]\n",
    "n_reps        = 100\n",
    "orig_n_cats   = 5\n",
    "new_n_cats    = 5\n",
    "\n",
    "sim_path = base_path + \"data/simulations/sim_2/\"\n",
    "Path(sim_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "for idx, factor_mul in enumerate(factor_mul_ls):\n",
    "    Path(sim_path + \"cond_\" + str(idx)).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    loadings, cov, intercepts = make_gen_params(loadings = ref_loadings,\n",
    "                                                intercepts = ref_intercepts,\n",
    "                                                orig_n_cats = orig_n_cats,\n",
    "                                                new_n_cats = new_n_cats,\n",
    "                                                cov = ref_cov,\n",
    "                                                factor_mul = factor_mul,\n",
    "                                                item_mul = item_mul_ls[idx])\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep + 1)\n",
    "        np.random.seed(rep + 1)\n",
    "\n",
    "        # Simulate data set.\n",
    "        data = sim_mirt(n_obs = n_obs_ls[idx],\n",
    "                        distribution = dist.MultivariateNormal(torch.zeros(cov.size(0)), cov),\n",
    "                        loadings = loadings,\n",
    "                        intercepts = intercepts,\n",
    "                        n_cats = [new_n_cats]*loadings.shape[0])[0]\n",
    "\n",
    "        # Save data set.\n",
    "        if idx == 0 and rep == 78:\n",
    "            np.savetxt(sim_path + \"cond_\" + str(idx) + \"/rep_\" + str(rep) + \".csv\",\n",
    "                       data.numpy(), \n",
    "                       delimiter = \",\")\n",
    "        else:\n",
    "            np.savetxt(sim_path + \"cond_\" + str(idx) + \"/rep_\" + str(rep) + \".gz\",\n",
    "                       data.numpy(), \n",
    "                       delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit amortized IWVI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_path = base_path + \"results/sim_2/aiwvi/\"\n",
    "\n",
    "# Set simulation cell parameters and regularization hyperparameter grid.\n",
    "n_obs_ls = [1000, 2000, 5000, 10000]\n",
    "n_reps   = 100\n",
    "\n",
    "for obs_idx, n_obs in enumerate(n_obs_ls):\n",
    "    # Print simulation condtion.\n",
    "    print(\"Starting replications for N = \" + str(n_obs))\n",
    "\n",
    "    # Get simulation condtion.\n",
    "    cond = str(int(obs_idx))\n",
    "\n",
    "    # Make directory to store results.\n",
    "    Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "    Path(res_path + \"cond_\" + cond).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "        print(\"Starting replication\", rep)\n",
    "\n",
    "        # Read in data.\n",
    "        if obs_idx == 0 and rep == 78:\n",
    "            data = pd.read_csv(base_path + \"data/simulations/sim_2/cond_\" + cond + \"/rep_\" + str(rep) + \".csv\",\n",
    "                   header = None,\n",
    "                   sep = \",\").to_numpy()\n",
    "        else:\n",
    "            data = pd.read_csv(base_path + \"data/simulations/sim_2/cond_\" + cond + \"/rep_\" + str(rep) + \".gz\",\n",
    "                               header = None,\n",
    "                               sep = \",\").to_numpy()\n",
    "        sim_loader = torch.utils.data.DataLoader(tensor_dataset(data),\n",
    "                                                 batch_size = 32,\n",
    "                                                 shuffle = True,\n",
    "                                                 **kwargs)\n",
    "\n",
    "        # Some hyperparameters for fitting.\n",
    "        n_cats = [5]*100\n",
    "        input_dim = 500\n",
    "        latent_dim = 10\n",
    "        inference_model_dims = [int((input_dim + 2 * latent_dim) / 2)]\n",
    "\n",
    "        # Fit model while ensuring numerical stability.\n",
    "        nan_output = True\n",
    "        seed = rep\n",
    "        while nan_output:\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Initialize model.\n",
    "            start = timeit.default_timer()\n",
    "            vae = MIRTVAEClass(input_dim = input_dim,\n",
    "                               inference_model_dims = inference_model_dims,\n",
    "                               latent_dim = latent_dim,\n",
    "                               n_cats = n_cats,\n",
    "                               learning_rate = 2.5e-3,\n",
    "                               device = device,\n",
    "                               log_interval = sim_log_interval,\n",
    "                               steps_anneal = 1000)\n",
    "\n",
    "            # Train model.\n",
    "            vae.run_training(sim_loader, sim_loader, iw_samples = 5)\n",
    "            stop = timeit.default_timer()\n",
    "\n",
    "            # Evaluate model.\n",
    "            elbo = vae.test(sim_loader,\n",
    "                            mc_samples = 1,\n",
    "                            iw_samples = 5,\n",
    "                            print_result = False)\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                nan_output = True\n",
    "                seed += 1\n",
    "            else:\n",
    "                nan_output = False\n",
    "\n",
    "        # Save run time.\n",
    "        Path(res_path + \"cond_\" + cond + \"/run_times\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/run_times/run_time_\" + str(rep) + \".txt\",\n",
    "                   np.array([stop - start]), \n",
    "                   delimiter = \",\",\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        # Save model loadings.\n",
    "        Path(res_path + \"cond_\" + cond + \"/loadings\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   vae.model.loadings.weight.data.numpy(),\n",
    "                   fmt='%f')\n",
    "\n",
    "        # Save model intercepts.\n",
    "        Path(res_path + \"cond_\" + cond + \"/ints\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/ints/ints_\" + str(rep) + \".txt\",\n",
    "                   vae.model.intercepts.bias.data.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        # Save model log-likelihood.\n",
    "        ll = -vae.bic(sim_loader,\n",
    "                      iw_samples = 100)[1] # I only use 100 IW samples to speed up the simulation.\n",
    "        Path(res_path + \"cond_\" + cond + \"/lls\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/lls/ll_\" + str(rep) + \".txt\",\n",
    "                   np.array([ll]), \n",
    "                   delimiter = \",\",\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        print(\"Stored results for replication\", rep)\n",
    "\n",
    "# Rotate factor loadings.\n",
    "subprocess.call(base_path + \"code/r/rotate_sim_2.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make boxplots of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings   = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/loadings.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_intercepts = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_cov        = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_loadings, ref_cov, ref_intercepts = make_gen_params(loadings = ref_loadings,\n",
    "                                                        intercepts = ref_intercepts,\n",
    "                                                        orig_n_cats = 5,\n",
    "                                                        new_n_cats = 5,\n",
    "                                                        cov = ref_cov,\n",
    "                                                        factor_mul = 2,\n",
    "                                                        item_mul = 1)\n",
    "\n",
    "# Make list to store amortized IWVI results.\n",
    "res_path = base_path + \"results/sim_2/aiwvi/\"\n",
    "aiwvi_cell_res_ls = []\n",
    "\n",
    "# Read in amortized IWVI results.\n",
    "for cond in range(4):\n",
    "    keys = [\"log_likelihood\", \"run_time\", \"loadings\", \"intercepts\", \"cov\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/rotated_loadings/rotated_loadings_\" + str(num) + \".txt\",\n",
    "                                       dtype = float) for num in range(100)]\n",
    "    cell_res[\"intercepts\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/ints/ints_\" + str(num) + \".txt\",\n",
    "                                         dtype = float) for num in range(100)]\n",
    "    cell_res[\"cov\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/covs/cov_\" + str(num) + \".txt\",\n",
    "                                  dtype = float) for num in range(100)]\n",
    "    cell_res[\"log_likelihood\"] = [-np.loadtxt(res_path + \"cond_\" + str(cond) + \"/lls/ll_\" + str(num) + \".txt\",\n",
    "                                             dtype = float).item() for num in range(100)]\n",
    "    cell_res[\"run_time\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/run_times/run_time_\" + str(num) + \".txt\",\n",
    "                                       dtype = float, skiprows = 1).sum() for num in range(100)]\n",
    "    \n",
    "    aiwvi_cell_res_ls.append(cell_res)\n",
    "\n",
    "# Make list to store MH-RM results.\n",
    "res_path = base_path + \"results/sim_2/mhrm/\"\n",
    "mhrm_cell_res_ls = []\n",
    "\n",
    "# Read in MH-RM results.\n",
    "for cond in range(4):\n",
    "    keys = [\"log_likelihood\", \"run_time\", \"loadings\", \"intercepts\", \"cov\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [unnormalize_loadings(np.loadtxt((res_path + \"cond_\" + str(cond) +\n",
    "                                                             \"/loadings/loadings_\" + str(num)),\n",
    "                                                            delimiter = \",\", dtype = float)) for\n",
    "                            num in range(100)]\n",
    "    cell_res[\"intercepts\"] = [-np.loadtxt(res_path + \"cond_\" + str(cond) + \"/ints/ints_\" + str(num),\n",
    "                                          delimiter = \",\", dtype = float) for num in range(100)]\n",
    "    # Accidentally saved T instead of Phi.\n",
    "    Ts = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/covs/cov_\" + str(num),\n",
    "                     delimiter = \",\", dtype = float) for num in range(100)]\n",
    "    cell_res[\"cov\"] = [np.matmul(T.transpose(), T) for T in Ts]\n",
    "    cell_res[\"log_likelihood\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/lls/ll_\" + str(num),\n",
    "                                             delimiter = \",\", dtype = float).item() for num in range(100)]\n",
    "    run_times = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/run_times/run_time_\" + str(num),\n",
    "                                       delimiter = \",\", skiprows = 1, dtype = float) for num in range(100)]\n",
    "    cell_res[\"run_time\"] = [sum([run_time[i] for i in (2, 3, 6)]) for run_time in run_times]\n",
    "    \n",
    "    mhrm_cell_res_ls.append(cell_res)\n",
    "\n",
    "# Fix intercepts.\n",
    "# NOTE: mirt does not report an intercept if a response category does not appear in the data.\n",
    "# I manually identified runs where certain response categories were missing and inserted NaNs\n",
    "# where appropriate.\n",
    "# for rep in [16, 17, 42, 80, 88, 89, 97]:\n",
    "#     # Read in data.\n",
    "#     data = pd.read_csv(path + \"data/simulations/sim_2/cond_0/rep_\" + str(rep) + \".gz\",\n",
    "#                    header = None,\n",
    "#                    sep = \",\")\n",
    "#     unique_vals = [data.iloc[:, col].unique() for col in data]\n",
    "#     if any([len(vals) != 2 for vals in unique_vals]):\n",
    "#         idxs = [len(vals) != 2 for vals in unique_vals].index(True)\n",
    "#         ints = mhrm_cell_res_ls[0][\"intercepts\"][rep].copy()\n",
    "#         temp_n_cats = [1] + [5]*100\n",
    "#         for idx in [idxs]:\n",
    "#             temp_ls = ints.tolist()\n",
    "#             temp_ls.insert(np.cumsum([n_cat - 1 for n_cat in temp_n_cats])[int(np.floor(idx / 5))], np.nan)\n",
    "#             ints = np.array(temp_ls)\n",
    "#         np.savetxt(path + \"results/sim_2/mhrm/cond_0/ints/ints_\" + str(rep),\n",
    "#                    ints,\n",
    "#                    delimiter = \",\",\n",
    "#                    fmt = \"%s\")\n",
    "# Read in MH-RM results, again.\n",
    "# for cond in range(4):\n",
    "#     keys = [\"log_likelihood\", \"run_time\", \"loadings\", \"intercepts\", \"cov\"]\n",
    "#     cell_res = {key : [] for key in keys}\n",
    "#     \n",
    "#     # Read results.\n",
    "#     cell_res[\"loadings\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/loadings/loadings_\" + str(num),\n",
    "#                                        delimiter = \",\", dtype = float) for num in range(100)]\n",
    "#     cell_res[\"intercepts\"] = [-np.loadtxt(res_path + \"cond_\" + str(cond) + \"/ints/ints_\" + str(num),\n",
    "#                                           delimiter = \",\", dtype = float) for num in range(100)]\n",
    "#     Ts = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/covs/cov_\" + str(num), # Accidentally saved T instead of Phi.\n",
    "#                      delimiter = \",\", dtype = float) for num in range(100)]\n",
    "#     cell_res[\"cov\"] = [np.matmul(T.transpose(), T) for T in Ts]\n",
    "#     cell_res[\"log_likelihood\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/lls/ll_\" + str(num),\n",
    "#                                              delimiter = \",\", dtype = float) for num in range(100)]\n",
    "#     cell_res[\"run_time\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/run_times/run_time_\" + str(num),\n",
    "#                                        delimiter = \",\", skiprows = 1, dtype = float) for num in range(100)]\n",
    "    \n",
    "#     mhrm_cell_res_ls.append(cell_res)\n",
    "    \n",
    "mse = mhrm_boxplots(aiwvi_cell_res_ls = aiwvi_cell_res_ls,\n",
    "                    mhrm_cell_res_ls = mhrm_cell_res_ls,\n",
    "                    cov = ref_cov,\n",
    "                    loadings = ref_loadings,\n",
    "                    intercepts = ref_intercepts,\n",
    "                    n_cats = [5]*100,\n",
    "                    ldgs_lims = [0, 0.0016],\n",
    "                    cov_lims = [0, 0.002],\n",
    "                    int_lims = [0, 0.025])\n",
    "mse.show()\n",
    "\n",
    "nums = [1000, 2000, 5000, 10000]\n",
    "times = comparison_time_plots(aiwvi_cell_res_ls,\n",
    "                              mhrm_cell_res_ls,\n",
    "                              [\"N = \" + f\"{num:,}\".replace(',', ' ') for num in nums],\n",
    "                              \"Amortized IWVI\", \"MH-RM\",\n",
    "                              \"Fitting Times for Amortized IWVI vs. MH-RM\")\n",
    "times.show()\n",
    "\n",
    "# Save plots to a single PDF.\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/mse_plots_sim_2.pdf\")\n",
    "pdf.savefig(mse, dpi = 300)\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/time_plot_sim_2.pdf\")\n",
    "pdf.savefig(times, dpi = 300)\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation 3: Comparing Estimator Efficiency in the Double Asymptotic Regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save simulated data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings   = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/loadings.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_intercepts = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/intercepts.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "ref_cov        = torch.from_numpy(np.loadtxt(base_path + \"data/simulations/lv_cov.txt\",\n",
    "                                  dtype = float,\n",
    "                                  delimiter = \",\")).float()\n",
    "\n",
    "factor_mul_ls = [2, 2, 2, 2]\n",
    "item_mul_ls   = [1, 2, 3, 4]\n",
    "n_obs_ls      = [2000, 10000, 50000, 100000]\n",
    "n_reps        = 100\n",
    "orig_n_cats   = 5\n",
    "new_n_cats    = 2\n",
    "\n",
    "sim_path = base_path + \"data/simulations/sim_3/\"\n",
    "Path(sim_path).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "for idx, factor_mul in enumerate(factor_mul_ls):\n",
    "    Path(sim_path + \"cond_\" + str(idx)).mkdir(parents = True, exist_ok = True)\n",
    "    \n",
    "    torch.manual_seed(1)\n",
    "    np.random.seed(1)\n",
    "    loadings, cov, intercepts = make_gen_params(loadings = ref_loadings,\n",
    "                                                intercepts = ref_intercepts,\n",
    "                                                orig_n_cats = orig_n_cats,\n",
    "                                                new_n_cats = new_n_cats,\n",
    "                                                cov = ref_cov,\n",
    "                                                factor_mul = factor_mul,\n",
    "                                                item_mul = item_mul_ls[idx])\n",
    "    \n",
    "    # Save generating parameters.\n",
    "    np.savetxt(sim_path + \"cond_\" + str(idx) + \"_gen_loadings.txt\",\n",
    "               loadings.numpy(),\n",
    "               fmt = \"%f\",\n",
    "               delimiter = \",\")\n",
    "    np.savetxt(sim_path + \"cond_\" + str(idx) + \"_gen_cov.txt\",\n",
    "           cov.numpy(),\n",
    "           fmt = \"%f\",\n",
    "           delimiter = \",\")\n",
    "    np.savetxt(sim_path + \"cond_\" + str(idx) + \"_gen_intercepts.txt\",\n",
    "           intercepts.numpy(),\n",
    "           fmt = \"%f\",\n",
    "           delimiter = \",\")\n",
    "    \n",
    "    for rep in range(n_reps):\n",
    "        # Set random seeds.\n",
    "        torch.manual_seed(rep + 1)\n",
    "        np.random.seed(rep + 1)\n",
    "\n",
    "        # Simulate data set.\n",
    "        data = sim_mirt(n_obs = n_obs_ls[idx],\n",
    "                        distribution = dist.MultivariateNormal(torch.zeros(cov.size(0)), cov),\n",
    "                        loadings = loadings,\n",
    "                        intercepts = intercepts,\n",
    "                        n_cats = [new_n_cats]*loadings.shape[0])[0]\n",
    "        \n",
    "        # Save data set.\n",
    "        if idx == 0 and rep == 45:\n",
    "            np.savetxt(sim_path + \"cond_\" + str(idx) + \"/rep_\" + str(rep) + \".csv\",\n",
    "               data.numpy(), \n",
    "               delimiter = \",\")\n",
    "        else:\n",
    "            np.savetxt(sim_path + \"cond_\" + str(idx) + \"/rep_\" + str(rep) + \".gz\",\n",
    "                       data.numpy(), \n",
    "                       delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit amortized IWVI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set simulation cell parameters and regularization hyperparameter grid.\n",
    "n_obs_ls      = [2000, 10000, 50000, 100000]\n",
    "item_mul_ls   = [1, 2, 3, 4]\n",
    "lr_ls         = [5e-3, 5e-3, 2.5e-3, 2.5e-3]\n",
    "n_reps        = 100\n",
    "\n",
    "for obs_idx, n_obs in enumerate(n_obs_ls):\n",
    "    # Print simulation condition.\n",
    "    print(\"Starting replications for N = \" + str(n_obs))\n",
    "\n",
    "    # Get simulation condition and learning rate.\n",
    "    cond = str(int(obs_idx))\n",
    "    lr = lr_ls[obs_idx]\n",
    "\n",
    "    # Make directory to store results.\n",
    "    res_path = base_path + \"results/sim_3/aiwvi/\"\n",
    "    Path(res_path).mkdir(parents = True, exist_ok = True)\n",
    "    Path(res_path + \"cond_\" + cond).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "        print(\"Starting replication\", rep)\n",
    "        # Read in data.\n",
    "        if obs_idx == 0 and rep == 45:\n",
    "            data = pd.read_csv(base_path + \"data/simulations/sim_3/cond_\" + cond + \"/rep_\" + str(rep) + \".csv\",\n",
    "                   header = None,\n",
    "                   sep = \",\").to_numpy()\n",
    "        else:\n",
    "            data = pd.read_csv(base_path + \"data/simulations/sim_3/cond_\" + cond + \"/rep_\" + str(rep) + \".gz\",\n",
    "                               header = None,\n",
    "                               sep = \",\").to_numpy()\n",
    "        sim_loader = torch.utils.data.DataLoader(tensor_dataset(data),\n",
    "                                                 batch_size = 32,\n",
    "                                                 shuffle = True,\n",
    "                                                 **kwargs)\n",
    "\n",
    "        # Some hyperparameters for fitting.\n",
    "        item_mul     = item_mul_ls[obs_idx]\n",
    "        n_cats       = [2] * 100 * item_mul\n",
    "        input_dim    = 200 * item_mul\n",
    "        latent_dim   = 10\n",
    "        inference_model_dims = [int((input_dim + 2 * latent_dim) / 2)]\n",
    "\n",
    "        # Fit model while ensuring numerical stability.\n",
    "        nan_output = True\n",
    "        seed = rep\n",
    "        while nan_output:\n",
    "            # Set random seeds.\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Initialize model.\n",
    "            start = timeit.default_timer()\n",
    "            vae = MIRTVAEClass(input_dim = input_dim,\n",
    "                               inference_model_dims = inference_model_dims,\n",
    "                               latent_dim = latent_dim,\n",
    "                               n_cats = n_cats,\n",
    "                               learning_rate = lr,\n",
    "                               device = device,\n",
    "                               log_interval = sim_log_interval,\n",
    "                               steps_anneal = 1000)\n",
    "\n",
    "            # Fit model.\n",
    "            vae.run_training(sim_loader, sim_loader, iw_samples = 5)\n",
    "            stop = timeit.default_timer()\n",
    "\n",
    "            # Evaluate model.\n",
    "            elbo = vae.test(sim_loader,\n",
    "                            mc_samples = 1,\n",
    "                            iw_samples = 5,\n",
    "                            print_result = False)\n",
    "\n",
    "            if np.isnan(elbo):\n",
    "                nan_output = True\n",
    "                seed += 1\n",
    "            else:\n",
    "                nan_output = False\n",
    "\n",
    "        # Save run time.\n",
    "        Path(res_path + \"cond_\" + cond + \"/run_times\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/run_times/run_time_\" + str(rep) + \".txt\",\n",
    "                   np.array([stop - start]), \n",
    "                   delimiter = \",\",\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        # Save model loadings.\n",
    "        Path(res_path + \"cond_\" + cond + \"/loadings\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/loadings/loadings_\" + str(rep) + \".txt\",\n",
    "                   vae.model.loadings.weight.data.numpy(),\n",
    "                   fmt='%f')\n",
    "\n",
    "        # Save model intercepts.\n",
    "        Path(res_path + \"cond_\" + cond + \"/ints\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/ints/ints_\" + str(rep) + \".txt\",\n",
    "                   vae.model.intercepts.bias.data.numpy(),\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        # Save factor scores.\n",
    "        Path(res_path + \"cond_\" + cond + \"/scores/\").mkdir(parents = True, exist_ok = True)\n",
    "        np.savetxt(res_path + \"cond_\" + cond + \"/scores/score_\" + str(rep) + \".txt\",\n",
    "                   vae.scores(sim_loader, mc_samples = 20, iw_samples = 5).numpy(),\n",
    "                   fmt = \"%f\")\n",
    "\n",
    "        print(\"Stored results for replication\", rep)\n",
    "\n",
    "# Rotate factor loadings.\n",
    "subprocess.call(base_path + \"code/r/rotate_sim_3.R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make boxplots of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generating parameters.\n",
    "ref_loadings_ls = []\n",
    "ref_intercepts_ls = []\n",
    "for cond in range(4):\n",
    "    ref_loadings_ls.append(torch.from_numpy(np.loadtxt((base_path + \"/data/simulations/sim_3/cond_\" +\n",
    "                                                        str(cond) + \"_gen_loadings.txt\"),\n",
    "                                                       dtype = float,\n",
    "                                                       delimiter = \",\")).float())\n",
    "    ref_intercepts_ls.append(torch.from_numpy(np.loadtxt((base_path + \"/data/simulations/sim_3/cond_\" +\n",
    "                                                          str(cond) + \"_gen_intercepts.txt\"),\n",
    "                                                         dtype = float,\n",
    "                                                         delimiter = \",\")).float())\n",
    "\n",
    "# Make list to store amortized IWVI results.\n",
    "res_path = base_path + \"results/sim_3/aiwvi/\"\n",
    "aiwvi_cell_res_ls = []\n",
    "\n",
    "# Read in amortized IWVI results.\n",
    "for cond in range(4):\n",
    "    keys = [\"log_likelihood\", \"run_time\", \"loadings\", \"intercepts\", \"cov\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/rotated_loadings/rotated_loadings_\" + str(num) + \".txt\",\n",
    "                                       dtype = float) for num in range(100)]\n",
    "    cell_res[\"intercepts\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/ints/ints_\" + str(num) + \".txt\",\n",
    "                                         dtype = float) for num in range(100)]\n",
    "    cell_res[\"run_time\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/run_times/run_time_\" + str(num) + \".txt\",\n",
    "                                       dtype = float, skiprows = 1).sum() for num in range(100)]\n",
    "    \n",
    "    aiwvi_cell_res_ls.append(cell_res)\n",
    "\n",
    "# Make list to store CJMLE results.\n",
    "res_path = base_path + \"results/sim_3/cjmle/\"\n",
    "cjmle_cell_res_ls = []\n",
    "\n",
    "# Read in CJMLE results.\n",
    "for cond in range(4):\n",
    "    keys = [\"log_likelihood\", \"run_time\", \"loadings\", \"intercepts\", \"cov\"]\n",
    "    cell_res = {key : [] for key in keys}\n",
    "    \n",
    "    # Read results.\n",
    "    cell_res[\"loadings\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/loadings/loadings_\" + str(num),\n",
    "                                       delimiter = \",\", dtype = float) for num in range(100)]\n",
    "    cell_res[\"intercepts\"] = [-np.loadtxt(res_path + \"cond_\" + str(cond) + \"/ints/ints_\" + str(num),\n",
    "                                         delimiter = \",\", dtype = float) for num in range(100)]\n",
    "    cell_res[\"run_time\"] = [np.loadtxt(res_path + \"cond_\" + str(cond) + \"/run_times/run_time_\" + str(num),\n",
    "                                       delimiter = \",\", skiprows = 1, dtype = float).sum() for num in range(100)]\n",
    "    \n",
    "    cjmle_cell_res_ls.append(cell_res)\n",
    "    \n",
    "mse = cjmle_boxplots(aiwvi_cell_res_ls = aiwvi_cell_res_ls,\n",
    "                     cjmle_cell_res_ls = cjmle_cell_res_ls,\n",
    "                     loadings_ls = ref_loadings_ls,\n",
    "                     intercepts_ls = ref_intercepts_ls)\n",
    "mse.show()\n",
    "\n",
    "n_obs_ls = [2000, 10000, 50000, 100000]\n",
    "n_items_ls = [100, 200, 300, 400]\n",
    "times = comparison_time_plots(aiwvi_cell_res_ls,\n",
    "                              cjmle_cell_res_ls,\n",
    "                              [\"N = \" + f\"{n_obs:,}\".replace(',', ' ') + \",\\nJ = \" + f\"{n_items:,}\".replace(',', ' ') for \n",
    "                               n_obs, n_items in zip(n_obs_ls, n_items_ls)],\n",
    "                              \"Amortized IWVI\", \"CJMLE\",\n",
    "                              \"Fitting Times for Amortized IWVI vs. CJMLE\")\n",
    "times.show()\n",
    "\n",
    "# Save plots to a single PDF.\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/mse_plots_sim_3.pdf\")\n",
    "pdf.savefig(mse, dpi = 300, bbox_inches = \"tight\")\n",
    "pdf.close()\n",
    "pdf = matplotlib.backends.backend_pdf.PdfPages(base_path + \"figures/time_plot_sim_3.pdf\")\n",
    "pdf.savefig(times, dpi = 300, bbox_inches = \"tight\")\n",
    "pdf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
